{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d624c1",
   "metadata": {
    "id": "25d624c1"
   },
   "source": [
    "# Homework 2: Language Modeling\n",
    "11-411/611 Natural Language Processing (Spring 2025)\n",
    "\n",
    "- RELEASED: Tuesday, Feb 18th, 2025\n",
    "- DUE: Thursday, March 13th 2025 11:59 pm EDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710365b",
   "metadata": {
    "id": "8710365b"
   },
   "source": [
    "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
    "\n",
    "In this assignment, you will be building your own language models powered by n-grams and RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485bc65",
   "metadata": {
    "id": "e485bc65"
   },
   "source": [
    "### Submission Guidelines\n",
    "**Programming:** \n",
    "- This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit `ngram_lm.py` and `rnn_lm.py` on Gradescope.\n",
    "- We recommended that you first code in the notebook and then copy the corresponding methods/classes to `ngram_lm.py` and `rnn_lm.py`.\n",
    "\n",
    "**Written:**\n",
    "- Analysis questions would require you to run your code.\n",
    "- You need to write your answers in a document and upload it alongside the programming components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wjvLk7XPMxnl",
   "metadata": {
    "id": "wjvLk7XPMxnl"
   },
   "source": [
    "### Upload (if using Colab) main.py and utils.py, and the data.zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bmEQByNzNI4d",
   "metadata": {
    "id": "bmEQByNzNI4d"
   },
   "outputs": [],
   "source": [
    "# !unzip data.zip\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f84134d",
   "metadata": {
    "id": "1f84134d"
   },
   "source": [
    "## Part 1: Language Models [60 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b3c6b",
   "metadata": {
    "id": "4d0b3c6b"
   },
   "source": [
    "### Step 0: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30667bf4",
   "metadata": {
    "id": "30667bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: requests in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from requests) (2024.12.14)\n",
      "Requirement already satisfied: torch in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\student\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install requests\n",
    "!pip install torch\n",
    "!pip install tqdm\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6eedf",
   "metadata": {
    "id": "b3a6eedf"
   },
   "source": [
    "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195db6a3",
   "metadata": {
    "id": "195db6a3"
   },
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea47b0",
   "metadata": {
    "id": "f4ea47b0"
   },
   "source": [
    "We have performed a round of preprocessing on the datasets.\n",
    "\n",
    "- Each file contains one sentence per line.\n",
    "- All punctuation marks have been removed.\n",
    "- Each line is a sequences of tokens separated by whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbe98cc",
   "metadata": {
    "id": "5fbe98cc"
   },
   "source": [
    "#### Special Symbols ( Already defined in `utils.py` )\n",
    "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed9e54f",
   "metadata": {
    "id": "9ed9e54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence START symbol: <s>\n",
      "Sentence END symbol: </s>\n",
      "Unknown word symbol: <UNK>\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence START symbol: {}\".format(START))\n",
    "print(\"Sentence END symbol: {}\".format(EOS))\n",
    "print(\"Unknown word symbol: {}\".format(UNK))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f4a6f",
   "metadata": {
    "id": "6b1f4a6f"
   },
   "source": [
    "#### Reading and processing an example file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60ce7c2",
   "metadata": {
    "id": "d60ce7c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n"
     ]
    }
   ],
   "source": [
    "# Read the sample file\n",
    "sample = read_file(\"data/sample.txt\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec373cc",
   "metadata": {
    "id": "4ec373cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>']\n",
      "['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
    "# Preprocessing example for bigrams (n=2)\n",
    "sample = preprocess(sample, n=3)\n",
    "for s in sample:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a2a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Flattens a nested list into a 1D list.\n",
    "flattened = flatten(sample)\n",
    "print(flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165b404",
   "metadata": {
    "id": "9165b404"
   },
   "source": [
    "### Step 1: N-Gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f73b0",
   "metadata": {
    "id": "5a4f73b0"
   },
   "source": [
    "#### TODO: Defining `get_ngrams()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138c35b6",
   "metadata": {
    "id": "138c35b6"
   },
   "outputs": [],
   "source": [
    "# #######################################\n",
    "# # TODO: get_ngrams()\n",
    "# #######################################\n",
    "# def get_ngrams(list_of_words, n):\n",
    "#     \"\"\"\n",
    "#     Returns a list of n-grams for a list of words.\n",
    "#     Args\n",
    "#     ----\n",
    "#     list_of_words: List[str]\n",
    "#         List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
    "#     n: int\n",
    "#         n-gram order e.g. 1, 2, 3\n",
    "    \n",
    "#     Returns:\n",
    "#         n_grams: List[Tuple]\n",
    "#             Returns a list containing n-gram tuples\n",
    "#     \"\"\"\n",
    "#     raise NotImplementedError\n",
    "\n",
    "# def get_ngrams(list_of_words, n):\n",
    "#     \"\"\"\n",
    "#     Returns a list of n-grams for a list of words.\n",
    "#     Args\n",
    "#     ----\n",
    "#     list_of_words: List[str]\n",
    "#         List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
    "#     n: int\n",
    "#         n-gram order e.g. 1, 2, 3\n",
    "    \n",
    "#     Returns:\n",
    "#         n_grams: List[Tuple]\n",
    "#             Returns a list containing n-gram tuples\n",
    "#     \"\"\"\n",
    "#     n_grams = []\n",
    "#     for i in range(len(list_of_words) - n + 1):\n",
    "#         n_gram = tuple(list_of_words[i:i+n])\n",
    "#         n_grams.append(n_gram)\n",
    "    \n",
    "#     return n_grams\n",
    "\n",
    "def get_ngrams(list_of_words, n):\n",
    "    \"\"\"\n",
    "    Returns a list of n-grams for a list of words.\n",
    "    \n",
    "    Args:\n",
    "    ----\n",
    "    list_of_words: List[str]\n",
    "        List of already preprocessed and flattened (1D) list of tokens.\n",
    "    n: int\n",
    "        n-gram order (e.g., 1 for unigrams, 2 for bigrams, etc.).\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    List[Tuple]\n",
    "        List of n-gram tuples.\n",
    "    \"\"\"\n",
    "    return [tuple(list_of_words[i:i+n]) for i in range(len(list_of_words) - n + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fdab35a",
   "metadata": {
    "id": "5fdab35a"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TEST: get_ngrams()\n",
    "#######################################\n",
    "sample = preprocess(read_file(\"sample.txt\"), n=3)\n",
    "flattened = flatten(sample)\n",
    "\n",
    "assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n",
    "        ('<s>', 'we', 'are'),\n",
    "        ('we', 'are', 'never'),\n",
    "        ('are', 'never', 'ever'),\n",
    "        ('never', 'ever', 'ever'),\n",
    "        ('ever', 'ever', 'ever'),\n",
    "        ('ever', 'ever', 'ever'),\n",
    "        ('ever', 'ever', 'getting'),\n",
    "        ('ever', 'getting', 'back'),\n",
    "        ('getting', 'back', 'together'),\n",
    "        ('back', 'together', '</s>'),\n",
    "        ('together', '</s>', '<s>'),\n",
    "        ('</s>', '<s>', '<s>'),\n",
    "        ('<s>', '<s>', 'we'),\n",
    "        ('<s>', 'we', 'are'),\n",
    "        ('we', 'are', 'the'),\n",
    "        ('are', 'the', 'ones'),\n",
    "        ('the', 'ones', 'together'),\n",
    "        ('ones', 'together', 'we'),\n",
    "        ('together', 'we', 'are'),\n",
    "        ('we', 'are', 'back'),\n",
    "        ('are', 'back', '</s>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffa1f0c-7d27-4fa9-857b-db47ee5033cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sample data: ['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n",
      "Preprocessed for bigrams: [['<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>'], ['<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']]\n",
      "Preprocessed for trigrams: [['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>'], ['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']]\n",
      "Flattened bigrams: ['<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n",
      "Flattened trigrams: ['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n",
      "Bigrams: [('<s>', 'we'), ('we', 'are'), ('are', 'never'), ('never', 'ever'), ('ever', 'ever'), ('ever', 'ever'), ('ever', 'ever'), ('ever', 'getting'), ('getting', 'back'), ('back', 'together'), ('together', '</s>'), ('</s>', '<s>'), ('<s>', 'we'), ('we', 'are'), ('are', 'the'), ('the', 'ones'), ('ones', 'together'), ('together', 'we'), ('we', 'are'), ('are', 'back'), ('back', '</s>')]\n",
      "Trigrams: [('<s>', '<s>', 'we'), ('<s>', 'we', 'are'), ('we', 'are', 'never'), ('are', 'never', 'ever'), ('never', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'ever'), ('ever', 'ever', 'getting'), ('ever', 'getting', 'back'), ('getting', 'back', 'together'), ('back', 'together', '</s>'), ('together', '</s>', '<s>'), ('</s>', '<s>', '<s>'), ('<s>', '<s>', 'we'), ('<s>', 'we', 'are'), ('we', 'are', 'the'), ('are', 'the', 'ones'), ('the', 'ones', 'together'), ('ones', 'together', 'we'), ('together', 'we', 'are'), ('we', 'are', 'back'), ('are', 'back', '</s>')]\n",
      "Unique bigrams: 16\n",
      "Unique trigrams: 19\n"
     ]
    }
   ],
   "source": [
    "# Read the sample file\n",
    "sample = read_file(\"sample.txt\")\n",
    "print(\"Raw sample data:\", sample)\n",
    "\n",
    "# Preprocess the content for bigrams (n=2)\n",
    "sample_bigrams = preprocess(sample, n=2)\n",
    "print(\"Preprocessed for bigrams:\", sample_bigrams)\n",
    "\n",
    "# Preprocess the content for trigrams (n=3)\n",
    "sample_trigrams = preprocess(sample, n=3)\n",
    "print(\"Preprocessed for trigrams:\", sample_trigrams)\n",
    "\n",
    "# Flatten the preprocessed data\n",
    "flattened_bigrams = flatten(sample_bigrams)\n",
    "print(\"Flattened bigrams:\", flattened_bigrams)\n",
    "\n",
    "flattened_trigrams = flatten(sample_trigrams)\n",
    "print(\"Flattened trigrams:\", flattened_trigrams)\n",
    "\n",
    "\n",
    "# Extract bigrams and trigrams\n",
    "bigrams = get_ngrams(flattened_bigrams, n=2)\n",
    "trigrams = get_ngrams(flattened_trigrams, n=3)\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count unique bigrams and trigrams\n",
    "unique_bigrams = len(Counter(bigrams))\n",
    "unique_trigrams = len(Counter(trigrams))\n",
    "\n",
    "print(\"Unique bigrams:\", unique_bigrams)\n",
    "print(\"Unique trigrams:\", unique_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbb4a88",
   "metadata": {
    "id": "7bbb4a88"
   },
   "source": [
    "#### **TODO:** Class `NGramLanguageModel()`\n",
    "\n",
    "*Now*, we will define our LanguageModel class.\n",
    "\n",
    "**Some Useful Variables:**\n",
    "- self.model: `dict` of n-grams and their corresponding probabilities, keys being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
    "- self.vocab: `dict` of unigram vocabulary with counts, keys being the words themselves and the values being their frequency.\n",
    "- self.n: `int` value for n-gram order (e.g. 1, 2, 3).\n",
    "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the language model\n",
    "- self.smoothing: `float` flag signifying the smoothing parameter.\n",
    "\n",
    "In `lm.py`, we will be taking most of these argumemts from the command line using this command:\n",
    "\n",
    "`python3 lm.py --train data/sample.txt --test data/sample.txt --n 3 --smoothing 0 --min_freq 1`\n",
    "\n",
    "Note that we will not be using log probabilities in this section. Store the probabilities as they are, not in log space.\n",
    "\n",
    "**Laplace Smoothing**\n",
    "\n",
    "There are two ways to perform this:\n",
    "- Either you calculate all possible n-grams at train time and calculate smooth probabilities for all of them, hence inflating the model (eager emoothing). You then use the probabilities as when required at test time. **OR**\n",
    "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use (lazy smoothing).\n",
    "\n",
    "You will be implementing lazy smoothing\n",
    "\n",
    "**Perplexity**\n",
    "\n",
    "Steps:\n",
    "1. Flatten the test data.\n",
    "2. Extract ngrams from the flattened data.\n",
    "3. Calculate perplexity according to given formula. For unseen n-grams, calculate using smoothed likelihood and store the unseen n-gram probability in the labguage model `model` attribute:\n",
    "\n",
    "$ppl(W_{test}) = ppl(W_1W_2 ... W_n)^{-1/n} $\n",
    "\n",
    "Tips:\n",
    "- Remember that product changes to summation under `log`. Take the log of probabilities, sum them up, and then exponentiate it to get back to the original scale.\n",
    "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams()`.\n",
    "- The test suite provided is **not exhaustive**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f2ce5c",
   "metadata": {
    "id": "24f2ce5c"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TODO: NGramLanguageModel()\n",
    "#######################################\n",
    "class NGramLanguageModel():\n",
    "    def __init__(self, n, train_data, alpha=1):\n",
    "        \"\"\"\n",
    "        Language model class.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        n: int\n",
    "            n-gram order (e.g., 1, 2, 3).\n",
    "        train_data: List[List]\n",
    "            Preprocessed training data in sentence format.\n",
    "        alpha: float\n",
    "            Smoothing parameter.\n",
    "\n",
    "        Attributes:\n",
    "        -----\n",
    "        self.tokens: Flattened list of tokens.\n",
    "        self.vocab: Dictionary of word counts.\n",
    "        self.model: Stores n-gram probabilities.\n",
    "        self.n_grams_counts: Dictionary of n-gram counts.\n",
    "        self.prefix_counts: Dictionary of (n-1)-gram counts.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.smoothing = alpha\n",
    "        self.train_data = train_data\n",
    "\n",
    "        # Preprocess data\n",
    "        self.tokens = flatten(train_data)\n",
    "        self.vocab = Counter(self.tokens)\n",
    "        self.n_grams_counts = Counter()\n",
    "        self.prefix_counts = Counter()\n",
    "        self.model = {}\n",
    "\n",
    "        # Construct model\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Computes n-gram and (n-1)-gram counts and stores initial probabilities.\n",
    "        \"\"\"\n",
    "        ngram_list = get_ngrams(self.tokens, self.n)\n",
    "\n",
    "        for ngram in ngram_list:\n",
    "            self.n_grams_counts[ngram] += 1\n",
    "            if self.n > 1:\n",
    "                prefix = ngram[:-1]\n",
    "                self.prefix_counts[prefix] += 1\n",
    "\n",
    "        # Store precomputed probabilities\n",
    "        for ngram in self.n_grams_counts.keys():\n",
    "            self.model[ngram] = self.get_prob(ngram)\n",
    "\n",
    "    def get_smooth_probabilities(self, ngrams):\n",
    "        \"\"\"\n",
    "        Returns smoothed probabilities for a list of n-grams.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        ngrams: list of tuples\n",
    "            List of n-gram tuples.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "        list of float\n",
    "            List of probabilities.\n",
    "        \"\"\"\n",
    "        return list(map(self.get_prob, ngrams))\n",
    "\n",
    "    def get_prob(self, ngram):\n",
    "        \"\"\"\n",
    "        Computes probability of an n-gram using Laplace smoothing.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        ngram: tuple\n",
    "            The n-gram tuple.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "        float\n",
    "            Probability of the n-gram.\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "        count = self.n_grams_counts.get(ngram, 0)\n",
    "\n",
    "        if self.n == 1:\n",
    "            return (count + self.smoothing) / (sum(self.vocab.values()) + self.smoothing * vocab_size)\n",
    "\n",
    "        prefix = ngram[:-1]\n",
    "        prefix_count = self.prefix_counts.get(prefix, 0)\n",
    "        return (count + self.smoothing) / (prefix_count + self.smoothing * vocab_size)\n",
    "\n",
    "    def perplexity(self, test_data):\n",
    "        \"\"\"\n",
    "        Computes perplexity using lazy Laplace smoothing.\n",
    "\n",
    "        Args:\n",
    "        -----\n",
    "        test_data: List[List]\n",
    "            Nested list of preprocessed test sentences.\n",
    "\n",
    "        Returns:\n",
    "        -----\n",
    "        float\n",
    "            Perplexity score.\n",
    "        \"\"\"\n",
    "        test_tokens = flatten(test_data)\n",
    "        test_ngrams = get_ngrams(test_tokens, self.n)\n",
    "\n",
    "        if not test_ngrams:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        log_prob_total = sum(math.log(self.model.get(ngram, self.get_prob(ngram)) + 1e-10) for ngram in test_ngrams)\n",
    "        return math.exp(-log_prob_total / len(test_ngrams))\n",
    "\n",
    "# class NGramLanguageModel():\n",
    "#     def __init__(self, n, train_data, alpha=1):\n",
    "#         \"\"\"\n",
    "#         Language model class implementing lazy Laplace smoothing.\n",
    "        \n",
    "#         Args\n",
    "#         ----\n",
    "#         n: int\n",
    "#             n-gram order\n",
    "#         train_data: List[List]\n",
    "#             Preprocessed unflattened list of sentences.\n",
    "#         alpha: float\n",
    "#             Smoothing parameter\n",
    "        \n",
    "#         Attributes\n",
    "#         ----------\n",
    "#         self.tokens: list\n",
    "#             Flattened list of tokens in training corpus.\n",
    "#         self.vocab: dict\n",
    "#             Vocabulary dictionary with counts.\n",
    "#         self.model: dict\n",
    "#             Stores n-gram probabilities.\n",
    "#         self.n_grams_counts: dict\n",
    "#             Stores frequency of n-grams in training data.\n",
    "#         self.prefix_counts: dict\n",
    "#             Stores frequency of (n-1)-grams in training data.\n",
    "#         \"\"\"\n",
    "#         self.n = n\n",
    "#         self.train_data = train_data\n",
    "#         self.smoothing = alpha\n",
    "#         self.tokens = flatten(train_data)  # Flatten train data\n",
    "#         self.vocab = Counter(self.tokens)  # Vocabulary with counts\n",
    "#         self.model = {}  # Stores n-gram probabilities\n",
    "#         self.n_grams_counts = Counter()\n",
    "#         self.prefix_counts = Counter()\n",
    "#         self.build()  # Build the model\n",
    "\n",
    "#     def build(self):\n",
    "#         \"\"\"\n",
    "#         Computes n-gram counts and prefix counts, then calculates probabilities for seen n-grams.\n",
    "#         \"\"\"\n",
    "#         ngrams = get_ngrams(self.tokens, self.n)\n",
    "#         for ngram in ngrams:\n",
    "#             self.n_grams_counts[ngram] += 1\n",
    "#             if self.n > 1:\n",
    "#                 self.prefix_counts[ngram[:-1]] += 1\n",
    "        \n",
    "#         # Compute probabilities for observed n-grams (Lazy Smoothing applies at test time)\n",
    "#         self.model = {ngram: self.get_prob(ngram) for ngram in self.n_grams_counts}\n",
    "\n",
    "#     def get_smooth_probabilities(self, ngrams):\n",
    "#         \"\"\"\n",
    "#         Returns a list of smoothed probabilities for a list of n-grams.\n",
    "\n",
    "#         Args\n",
    "#         ----\n",
    "#         ngrams: list of tuples\n",
    "#             List of n-grams for which probabilities are required.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         list\n",
    "#             List of smoothed probabilities.\n",
    "#         \"\"\"\n",
    "#         return [self.get_prob(ngram) for ngram in ngrams]\n",
    "\n",
    "#     def get_prob(self, ngram):\n",
    "#         \"\"\"\n",
    "#         Returns the probability of an n-gram using Laplace smoothing.\n",
    "\n",
    "#         Args\n",
    "#         ----\n",
    "#         ngram: tuple\n",
    "#             The n-gram tuple.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         float\n",
    "#             The probability of the given n-gram.\n",
    "#         \"\"\"\n",
    "#         vocab_size = len(self.vocab)\n",
    "#         count = self.n_grams_counts.get(ngram, 0)\n",
    "\n",
    "#         if self.n == 1:  # Unigram case\n",
    "#             total_tokens = len(self.tokens)\n",
    "#             return (count + self.smoothing) / (total_tokens + self.smoothing * vocab_size)\n",
    "\n",
    "#         prefix = ngram[:-1]\n",
    "#         prefix_count = self.prefix_counts.get(prefix, 0)\n",
    "#         return (count + self.smoothing) / (prefix_count + self.smoothing * vocab_size)\n",
    "\n",
    "#     def perplexity(self, test_data):\n",
    "#         \"\"\"\n",
    "#         Computes perplexity for the given test data using lazy smoothing.\n",
    "\n",
    "#         Args\n",
    "#         ----\n",
    "#         test_data: List[List]\n",
    "#             Preprocessed nested list of sentences.\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         float\n",
    "#             Perplexity value.\n",
    "#         \"\"\"\n",
    "#         test_tokens = flatten(test_data)\n",
    "#         test_ngrams = get_ngrams(test_tokens, self.n)\n",
    "\n",
    "#         prob_product = 1.0\n",
    "#         total_ngrams = len(test_ngrams)\n",
    "\n",
    "#         for ngram in test_ngrams:\n",
    "#             if ngram not in self.model:  # Lazy smoothing: compute probability for unseen n-grams\n",
    "#                 self.model[ngram] = self.get_prob(ngram)\n",
    "#             prob_product *= self.model[ngram]  # Multiply raw probabilities\n",
    "\n",
    "#         # Compute perplexity correctly\n",
    "#         if total_ngrams == 0:\n",
    "#             return float(\"inf\")  # Edge case: No n-grams found\n",
    "        \n",
    "#         perplexity = pow(prob_product, -1 / total_ngrams)  # Using correct formula\n",
    "#         return perplexity\n",
    "\n",
    "\n",
    "# # class NGramLanguageModel():\n",
    "# #     def __init__(self, n, train_data, alpha=1):\n",
    "# #         self.n = n\n",
    "# #         self.train_data = train_data\n",
    "# #         self.smoothing = alpha\n",
    "# #         self.tokens = flatten(train_data)  # Flatten train data\n",
    "# #         self.vocab = Counter(self.tokens)  # Vocabulary with counts\n",
    "# #         self.model = {}  # Stores n-gram probabilities\n",
    "# #         self.n_grams_counts = Counter()\n",
    "# #         self.prefix_counts = Counter()\n",
    "# #         self.build()  # Build the model\n",
    "\n",
    "# #     def build(self):\n",
    "# #         \"\"\"Precomputes probabilities for observed n-grams only.\"\"\"\n",
    "# #         ngrams = get_ngrams(self.tokens, self.n)\n",
    "# #         for ngram in ngrams:\n",
    "# #             self.n_grams_counts[ngram] += 1\n",
    "# #             if self.n > 1:\n",
    "# #                 self.prefix_counts[ngram[:-1]] += 1\n",
    "# #         # Compute probabilities for **seen** n-grams (Lazy Smoothing happens at test time)\n",
    "# #         self.model = {ngram: self.get_prob(ngram) for ngram in self.n_grams_counts}\n",
    "\n",
    "# #     def get_smooth_probabilities(self, ngrams):\n",
    "# #         \"\"\"Returns a list of probabilities for a list of n-grams.\"\"\"\n",
    "# #         return [self.get_prob(ngram) for ngram in ngrams]\n",
    "\n",
    "# #     def get_prob(self, ngram):\n",
    "# #         \"\"\"Returns the probability of an n-gram, applying Laplace smoothing.\"\"\"\n",
    "# #         vocab_size = len(self.vocab)\n",
    "# #         count = self.n_grams_counts.get(ngram, 0)\n",
    "        \n",
    "# #         if self.n == 1:  # Unigram model\n",
    "# #             total_tokens = len(self.tokens)\n",
    "# #             return (count + self.smoothing) / (total_tokens + self.smoothing * vocab_size)\n",
    "        \n",
    "# #         prefix = ngram[:-1]\n",
    "# #         prefix_count = self.prefix_counts.get(prefix, 0)\n",
    "# #         return (count + self.smoothing) / (prefix_count + self.smoothing * vocab_size)\n",
    "\n",
    "# #     def perplexity(self, test_data):\n",
    "# #         \"\"\"Computes perplexity using the correct formula & lazy smoothing.\"\"\"\n",
    "# #         test_tokens = flatten(test_data)\n",
    "# #         test_ngrams = get_ngrams(test_tokens, self.n)\n",
    "        \n",
    "# #         log_prob_sum = 0.0\n",
    "# #         for ngram in test_ngrams:\n",
    "# #             if ngram not in self.model:  # Apply Lazy Smoothing for unseen n-grams\n",
    "# #                 self.model[ngram] = self.get_prob(ngram)\n",
    "# #             log_prob_sum += math.log(self.model[ngram] + 1e-10)  # Avoid log(0)\n",
    "\n",
    "# #         N = len(test_ngrams)\n",
    "# #         return math.exp(-log_prob_sum / max(1, N)) if N > 0 else float('inf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909b9c4a",
   "metadata": {
    "id": "909b9c4a"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TEST: NGramLanguageModel()\n",
    "#######################################\n",
    "# For the sake of understanding we will pass alpha as 0 (no smoothing), so that you gain intuition about the probabilities\n",
    "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
    "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
    "\n",
    "expected_vocab = Counter({'<s>': 2,\n",
    "        'we': 3,\n",
    "        'are': 3,\n",
    "        'never': 1,\n",
    "        'ever': 4,\n",
    "        'getting': 1,\n",
    "        'back': 2,\n",
    "        'together': 2,\n",
    "        '</s>': 2,\n",
    "        'the': 1,\n",
    "        'ones': 1})\n",
    "\n",
    "expected_model = {('<s>', 'we'): 1.0,\n",
    "        ('we', 'are'): 1.0,\n",
    "        ('are', 'never'): 0.3333333333333333,\n",
    "        ('never', 'ever'): 1.0,\n",
    "        ('ever', 'ever'): 0.75,\n",
    "        ('ever', 'getting'): 0.25,\n",
    "        ('getting', 'back'): 1.0,\n",
    "        ('back', 'together'): 0.5,\n",
    "        ('together', '</s>'): 0.5,\n",
    "        ('</s>', '<s>'): 1.0,\n",
    "        ('are', 'the'): 0.3333333333333333,\n",
    "        ('the', 'ones'): 1.0,\n",
    "        ('ones', 'together'): 1.0,\n",
    "        ('together', 'we'): 0.5,\n",
    "        ('are', 'back'): 0.3333333333333333,\n",
    "        ('back', '</s>'): 0.5}\n",
    "\n",
    "assert test_lm.vocab == expected_vocab, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
    "\n",
    "assert test_lm.model == expected_model, (\n",
    "    f\"Model mismatch! \\n\"\n",
    "    f\"Expected keys but missing: {set(expected_model.keys()) - set(test_lm.model.keys())}\\n\"\n",
    "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model.keys())}\\n\"\n",
    "    f\"Discrepancies in probabilities: \"\n",
    "    f\"{ {k: (expected_model[k], test_lm.model[k]) for k in expected_model if k in test_lm.model and expected_model[k] != test_lm.model[k]} }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501ac225",
   "metadata": {
    "id": "501ac225"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TEST smoothing: NGramLanguageModel()\n",
    "#######################################\n",
    "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
    "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
    "\n",
    "expected_vocab_smoothing = Counter({'<s>': 2,\n",
    "        'we': 3,\n",
    "        'are': 3,\n",
    "        'never': 1,\n",
    "        'ever': 4,\n",
    "        'getting': 1,\n",
    "        'back': 2,\n",
    "        'together': 2,\n",
    "        '</s>': 2,\n",
    "        'the': 1,\n",
    "        'ones': 1})\n",
    "\n",
    "expected_model_smoothing ={('<s>', 'we'): 0.23076923076923078,\n",
    "        ('we', 'are'): 0.2857142857142857,\n",
    "        ('are', 'never'): 0.14285714285714285,\n",
    "        ('never', 'ever'): 0.16666666666666666,\n",
    "        ('ever', 'ever'): 0.26666666666666666,\n",
    "        ('ever', 'getting'): 0.13333333333333333,\n",
    "        ('getting', 'back'): 0.16666666666666666,\n",
    "        ('back', 'together'): 0.15384615384615385,\n",
    "        ('together', '</s>'): 0.15384615384615385,\n",
    "        ('</s>', '<s>'): 0.16666666666666666,\n",
    "        ('are', 'the'): 0.14285714285714285,\n",
    "        ('the', 'ones'): 0.16666666666666666,\n",
    "        ('ones', 'together'): 0.16666666666666666,\n",
    "        ('together', 'we'): 0.15384615384615385,\n",
    "        ('are', 'back'): 0.14285714285714285,\n",
    "        ('back', '</s>'): 0.15384615384615385}\n",
    "\n",
    "\n",
    "assert test_lm.vocab == expected_vocab_smoothing, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
    "\n",
    "assert test_lm.model == expected_model_smoothing, (\n",
    "    f\"Model mismatch! \\n\"\n",
    "    f\"Expected keys but missing: {set(expected_model_smoothing.keys()) - set(test_lm.model.keys())}\\n\"\n",
    "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_smoothing.keys())}\\n\"\n",
    "    f\"Discrepancies in probabilities: \"\n",
    "    f\"{ {k: (expected_model_smoothing[k], test_lm.model[k]) for k in expected_model_smoothing if k in test_lm.model and expected_model_smoothing[k] != test_lm.model[k]} }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157b0749",
   "metadata": {
    "id": "157b0749"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "# TEST unigram: NGramLanguageModel()\n",
    "#######################################\n",
    "sample = preprocess(read_file(\"data/sample.txt\"), n=1)\n",
    "test_lm = NGramLanguageModel(n=1, train_data=sample, alpha=1)\n",
    "\n",
    "expected_vocab_unigram = Counter({'<s>': 2,\n",
    "        'we': 3,\n",
    "        'are': 3,\n",
    "        'never': 1,\n",
    "        'ever': 4,\n",
    "        'getting': 1,\n",
    "        'back': 2,\n",
    "        'together': 2,\n",
    "        '</s>': 2,\n",
    "        'the': 1,\n",
    "        'ones': 1})\n",
    "\n",
    "expected_model_unigram = {('<s>',): 0.09090909090909091,\n",
    "        ('we',): 0.12121212121212122,\n",
    "        ('are',): 0.12121212121212122,\n",
    "        ('never',): 0.06060606060606061,\n",
    "        ('ever',): 0.15151515151515152,\n",
    "        ('getting',): 0.06060606060606061,\n",
    "        ('back',): 0.09090909090909091,\n",
    "        ('together',): 0.09090909090909091,\n",
    "        ('</s>',): 0.09090909090909091,\n",
    "        ('the',): 0.06060606060606061,\n",
    "        ('ones',): 0.06060606060606061}\n",
    "\n",
    "\n",
    "assert test_lm.vocab == expected_vocab_unigram, f\"Vocabulary mismatch! Expected: {expected_vocab}, but got: {test_lm.vocab}\"\n",
    "\n",
    "assert test_lm.model == expected_model_unigram, (\n",
    "    f\"Model mismatch! \\n\"\n",
    "    f\"Expected keys but missing: {set(expected_model_unigram.keys()) - set(test_lm.model.keys())}\\n\"\n",
    "    f\"Unexpected keys in model: {set(test_lm.model.keys()) - set(expected_model_unigram.keys())}\\n\"\n",
    "    f\"Discrepancies in probabilities: \"\n",
    "    f\"{ {k: (expected_model_unigram[k], test_lm.model[k]) for k in expected_model_unigram if k in test_lm.model and expected_model_unigram[k] != test_lm.model[k]} }\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2927e9aa",
   "metadata": {
    "id": "2927e9aa"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m test_lm \u001b[38;5;241m=\u001b[39m NGramLanguageModel(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, train_data\u001b[38;5;241m=\u001b[39msample, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m test_ppl \u001b[38;5;241m=\u001b[39m test_lm\u001b[38;5;241m.\u001b[39mperplexity(sample)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# TEST: perplexity()\n",
    "#######################################\n",
    "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
    "test_ppl = test_lm.perplexity(sample)\n",
    "assert test_ppl < 1.7\n",
    "assert test_ppl > 0\n",
    "\n",
    "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
    "test_ppl = test_lm.perplexity(sample)\n",
    "assert test_ppl < 5.0\n",
    "assert test_ppl > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fda0b2",
   "metadata": {},
   "source": [
    "### Step 2: RNN Language Model\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. Unlike traditional neural networks, which assume independence among inputs, RNNs utilize their internal state (memory) to process sequences of inputs. This makes them particularly well-suited for tasks where context and order matter.\n",
    "\n",
    "Before diving into building RNN Language Models using PyTorch, it's essential to have a solid foundation in the following areas:\n",
    ". We assume you have had a basic understanding of PyTorch and its core concepts, including tensors, autograd, modules (nn.Module), and how to construct simple neural networks using PyTorch. For more comprehensive learning, refer to the [PyTorch official tutorials](https://pytorch.org/tutorials/) and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454617c7",
   "metadata": {},
   "source": [
    "#### Preparing the Data\n",
    "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks. You can download the 50d embeddings for this assignment from [Canvas](https://canvas.cmu.edu/courses/39596/files/10855662?module_item_id=5748476).\n",
    "\n",
    "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
    "\n",
    "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
    "\n",
    "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4411999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9b6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    embeddings_dict = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Path to the GloVe file\n",
    "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
    "    vocab_size = len(word_to_ix)\n",
    "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "    for word, ix in word_to_ix.items():\n",
    "        if word in embeddings_dict:\n",
    "            embedding_matrix[ix] = embeddings_dict[word]\n",
    "        else:\n",
    "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
    "    return embedding_matrix\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbade19",
   "metadata": {},
   "source": [
    "#### TODO: Defining the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc88721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
    "        \"\"\"\n",
    "        RNN model class.\n",
    "        \n",
    "        Args\n",
    "        ____\n",
    "        vocab_size: int\n",
    "            Size of the vocabulary\n",
    "        embedding_dim: int\n",
    "            Dimension of the word embeddings\n",
    "        hidden_dim: int\n",
    "            Dimension of the hidden state of the RNN\n",
    "        embedding_matrix: torch.Tensor\n",
    "            Pre-trained GloVe embeddings\n",
    "        \"\"\"\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() \n",
    "                                 else \"cuda\" if torch.cuda.is_available() \n",
    "                                 else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer initialized with GloVe embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False  # Freeze the embeddings\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        The forward pass of the RNN model.\n",
    "        \n",
    "        Args\n",
    "        ____\n",
    "        x: torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length)\n",
    "        hidden: torch.Tensor\n",
    "            Hidden state tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out: torch.Tensor\n",
    "            Output tensor of shape (batch_size, sequence_length, vocab_size)\n",
    "        hidden: torch.Tensor\n",
    "            Hidden state tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device)\n",
    "        if hidden is None:\n",
    "            # Initialize hidden state if not provided\n",
    "            hidden = torch.zeros(1, x.size(0), self.hidden_dim).to(self.device)\n",
    "        else:\n",
    "            hidden = hidden.to(self.device)\n",
    "\n",
    "        # Embedding lookup\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # RNN forward pass\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
    "        \"\"\"\n",
    "        Predicts the next words given a sequence.\n",
    "        \n",
    "        Args\n",
    "        ____\n",
    "        sequence: str\n",
    "            Input sequence\n",
    "        word_to_ix: dict\n",
    "            Dictionary mapping words to their corresponding indices\n",
    "        ix_to_word: dict\n",
    "            Dictionary mapping indices to their corresponding words\n",
    "        num_words: int\n",
    "            Maximum number of words to predict\n",
    "        mode: str\n",
    "            Mode of prediction. 'max' or 'multinomial'\n",
    "            'max' mode selects the word with maximum probability\n",
    "            'multinomial' mode samples the word from the probability distribution\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted_sequence: List[str]\n",
    "            List of predicted words (excluding the initial sequence)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        predicted_sequence = []\n",
    "\n",
    "        # Convert the input sequence to indices, handling unknown words\n",
    "        sequence_indices = [word_to_ix.get(word, word_to_ix['<UNK>']) for word in sequence.split()]\n",
    "        sequence_tensor = torch.tensor(sequence_indices, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "\n",
    "        hidden = None\n",
    "        for _ in range(num_words):\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                output, hidden = self.forward(sequence_tensor, hidden)\n",
    "                output = output[:, -1, :]  # Get the last output\n",
    "\n",
    "                # Predict the next word\n",
    "                if mode == 'max':\n",
    "                    _, next_word_idx = torch.max(output, dim=1)\n",
    "                elif mode == 'multinomial':\n",
    "                    probs = F.softmax(output, dim=1)\n",
    "                    next_word_idx = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "                # Get the predicted word\n",
    "                next_word = ix_to_word.get(next_word_idx.item(), '<UNK>')\n",
    "                predicted_sequence.append(next_word)\n",
    "\n",
    "                # Update the sequence tensor with the predicted word\n",
    "                sequence_tensor = torch.tensor([[next_word_idx.item()]], dtype=torch.long).to(self.device)\n",
    "\n",
    "        return predicted_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c516d",
   "metadata": {},
   "source": [
    "#### Training the Model\n",
    "The following code snippet provided is responsible for training the RNN language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96135209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/10, Loss: 2.489769220352173, Perplexity: 12.058492944551825\n",
      "Epoch 2/10, Loss: 2.3438196182250977, Perplexity: 10.42096474805513\n",
      "Epoch 3/10, Loss: 2.212952136993408, Perplexity: 9.142666997991427\n",
      "Epoch 4/10, Loss: 2.0943455696105957, Perplexity: 8.12012517431332\n",
      "Epoch 5/10, Loss: 1.985386848449707, Perplexity: 7.281863818381051\n",
      "Epoch 6/10, Loss: 1.883654236793518, Perplexity: 6.577496730070404\n",
      "Epoch 7/10, Loss: 1.7871267795562744, Perplexity: 5.972268148099225\n",
      "Epoch 8/10, Loss: 1.694391131401062, Perplexity: 5.443330682903116\n",
      "Epoch 9/10, Loss: 1.6047165393829346, Perplexity: 4.976448775551447\n",
      "Epoch 10/10, Loss: 1.5180013179779053, Perplexity: 4.5630958971646205\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# TEST: RNNLanguageModel() and training\n",
    "#######################################\n",
    "torch.manual_seed(11411)\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
    "\n",
    "lines = \"\"\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(RNN.device)\n",
    "        targets = targets.to(RNN.device)\n",
    "        \n",
    "        RNN.zero_grad()\n",
    "        output, _ = RNN(inputs)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
    "    lines += line + \"\\n\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba8069",
   "metadata": {
    "id": "14ba8069"
   },
   "source": [
    "## Part 2: Written [40 points]. We have given some code for some of the written parts to make it easier for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc59cd52-a7e7-462a-a0c0-3deb1d7332f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the datasets\n",
    "business_data = preprocess(read_file(\"business.txt\"), n=2)\n",
    "sports_data = preprocess(read_file(\"sport.txt\"), n=2)\n",
    "\n",
    "# Train the models\n",
    "business_bigram_model = NGramLanguageModel(n=2, train_data=business_data, alpha=1)\n",
    "business_trigram_model = NGramLanguageModel(n=3, train_data=business_data, alpha=1)\n",
    "\n",
    "sports_bigram_model = NGramLanguageModel(n=2, train_data=sports_data, alpha=1)\n",
    "sports_trigram_model = NGramLanguageModel(n=3, train_data=sports_data, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "021a2e69-78fd-49b7-9be0-79c6d660ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique n-grams\n",
    "unique_business_bigrams = len(business_bigram_model.n_grams_counts)\n",
    "unique_business_trigrams = len(business_trigram_model.n_grams_counts)\n",
    "\n",
    "unique_sports_bigrams = len(sports_bigram_model.n_grams_counts)\n",
    "unique_sports_trigrams = len(sports_trigram_model.n_grams_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf522508-82e3-42ed-8923-bffcc0375d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for unique business bigrams: 83819\n",
      "for unique business trigrams: 141220\n",
      "for unique sports bigrams: 77398\n",
      "for unique sports trigrams: 135644\n"
     ]
    }
   ],
   "source": [
    "print(\"for unique business bigrams: \" + str(unique_business_bigrams))\n",
    "print(\"for unique business trigrams: \" + str(unique_business_trigrams))\n",
    "print(\"for unique sports bigrams: \" + str(unique_sports_bigrams))\n",
    "print(\"for unique sports trigrams: \" + str(unique_sports_trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8a601f-478d-47fd-bea7-e5382ffe7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size for business dataset\n",
    "vocab_size_business = len(business_bigram_model.vocab)\n",
    "\n",
    "# Vocabulary size for sports dataset\n",
    "vocab_size_sports = len(sports_bigram_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6319e3c-0f40-476f-83fe-c24b9eb6aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible n-grams for business dataset\n",
    "possible_bigrams_business = vocab_size_business ** 2\n",
    "possible_trigrams_business = vocab_size_business ** 3\n",
    "\n",
    "# Possible n-grams for sports dataset\n",
    "possible_bigrams_sports = vocab_size_sports ** 2\n",
    "possible_trigrams_sports = vocab_size_sports ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c62d031-55ea-46ed-b9f1-c1776ea513a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Dataset:\n",
      "Unique bigrams: 83819 (out of 141991056 possible bigrams)\n",
      "Unique trigrams: 141220 (out of 1691965423296 possible trigrams)\n",
      "\n",
      "Sports Dataset:\n",
      "Unique bigrams: 77398 (out of 112508449 possible bigrams)\n",
      "Unique trigrams: 135644 (out of 1193377118543 possible trigrams)\n"
     ]
    }
   ],
   "source": [
    "# Comparison for business dataset\n",
    "print(\"Business Dataset:\")\n",
    "print(f\"Unique bigrams: {unique_business_bigrams} (out of {possible_bigrams_business} possible bigrams)\")\n",
    "print(f\"Unique trigrams: {unique_business_trigrams} (out of {possible_trigrams_business} possible trigrams)\")\n",
    "\n",
    "# Comparison for sports dataset\n",
    "print(\"\\nSports Dataset:\")\n",
    "print(f\"Unique bigrams: {unique_sports_bigrams} (out of {possible_bigrams_sports} possible bigrams)\")\n",
    "print(f\"Unique trigrams: {unique_sports_trigrams} (out of {possible_trigrams_sports} possible trigrams)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53431996",
   "metadata": {
    "id": "53431996"
   },
   "source": [
    "### **Written 4.2** – Song Attribution [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb692ad",
   "metadata": {},
   "source": [
    "The cell below contains helper code to create an n-gram model for song attribution. You can use this code to train a model on the file provided in the train variable and compute the model's perplexity on the test lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4751ea5b",
   "metadata": {
    "id": "4751ea5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.00661898624173\n"
     ]
    }
   ],
   "source": [
    "# Example code for Taylor Swift N-Gram LM\n",
    "n = 3\n",
    "smoothing = 0.1\n",
    "min_freq = 1\n",
    "\n",
    "train = read_file(\"taylor_swift.txt\")\n",
    "test = read_file(\"test_lyrics.txt\")\n",
    "\n",
    "train = preprocess(train, n)\n",
    "test = preprocess(test, n)\n",
    "lm = NGramLanguageModel(n, train, smoothing)\n",
    "\n",
    "ppl = lm.perplexity(test)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c79546-a8b2-4620-8498-f3a22e1ea295",
   "metadata": {},
   "source": [
    "## Task 1: Compute Perplexity Scores for Tri-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98c62b80-46e5-47d0-9629-d6c4bdf97351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity scores for tri-gram models:\n",
      "(a) Taylor Swift: 138.00661898624173\n",
      "(b) Green Day: 522.5399979005886\n",
      "(c) Ed Sheeran: 521.2573608668955\n",
      "Most likely lyricist (tri-gram): Taylor Swift\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the unattributed song\n",
    "test_lyrics = preprocess(read_file(\"test_lyrics.txt\"), n=3)\n",
    "\n",
    "# Train tri-gram models for each artist\n",
    "taylor_swift_train = preprocess(read_file(\"taylor_swift.txt\"), n=3)\n",
    "green_day_train = preprocess(read_file(\"green_day.txt\"), n=3)\n",
    "ed_sheeran_train = preprocess(read_file(\"ed_sheeran.txt\"), n=3)\n",
    "\n",
    "taylor_swift_lm = NGramLanguageModel(n=3, train_data=taylor_swift_train, alpha=0.1)\n",
    "green_day_lm = NGramLanguageModel(n=3, train_data=green_day_train, alpha=0.1)\n",
    "ed_sheeran_lm = NGramLanguageModel(n=3, train_data=ed_sheeran_train, alpha=0.1)\n",
    "\n",
    "# Compute perplexity for each artist\n",
    "taylor_swift_ppl = taylor_swift_lm.perplexity(test_lyrics)\n",
    "green_day_ppl = green_day_lm.perplexity(test_lyrics)\n",
    "ed_sheeran_ppl = ed_sheeran_lm.perplexity(test_lyrics)\n",
    "\n",
    "print(\"Perplexity scores for tri-gram models:\")\n",
    "print(f\"(a) Taylor Swift: {taylor_swift_ppl}\")\n",
    "print(f\"(b) Green Day: {green_day_ppl}\")\n",
    "print(f\"(c) Ed Sheeran: {ed_sheeran_ppl}\")\n",
    "\n",
    "# Identify the most likely lyricist\n",
    "most_likely_artist = min(\n",
    "    [\"Taylor Swift\", \"Green Day\", \"Ed Sheeran\"],\n",
    "    key=lambda x: taylor_swift_ppl if x == \"Taylor Swift\" else green_day_ppl if x == \"Green Day\" else ed_sheeran_ppl\n",
    ")\n",
    "print(f\"Most likely lyricist (tri-gram): {most_likely_artist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b72a3f7-ba8e-4422-ae44-5eb6c6125fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Perplexity scores for tri-gram models:\n",
      "   (a) Taylor Swift: 138.00661898624173\n",
      "   (b) Green Day: 522.5399979005886\n",
      "   (c) Ed Sheeran: 521.2573608668955\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the unattributed song (Tri-Gram)\n",
    "test_lyrics = preprocess(read_file(\"test_lyrics.txt\"), n=3)\n",
    "\n",
    "# Train tri-gram models for each artist\n",
    "taylor_swift_train = preprocess(read_file(\"taylor_swift.txt\"), n=3)\n",
    "green_day_train = preprocess(read_file(\"green_day.txt\"), n=3)\n",
    "ed_sheeran_train = preprocess(read_file(\"ed_sheeran.txt\"), n=3)\n",
    "\n",
    "taylor_swift_lm = NGramLanguageModel(n=3, train_data=taylor_swift_train, alpha=0.1)\n",
    "green_day_lm = NGramLanguageModel(n=3, train_data=green_day_train, alpha=0.1)\n",
    "ed_sheeran_lm = NGramLanguageModel(n=3, train_data=ed_sheeran_train, alpha=0.1)\n",
    "\n",
    "# Compute perplexity for each artist\n",
    "taylor_swift_ppl = taylor_swift_lm.perplexity(test_lyrics)\n",
    "green_day_ppl = green_day_lm.perplexity(test_lyrics)\n",
    "ed_sheeran_ppl = ed_sheeran_lm.perplexity(test_lyrics)\n",
    "\n",
    "print(\"1. Perplexity scores for tri-gram models:\")\n",
    "print(f\"   (a) Taylor Swift: {taylor_swift_ppl}\")\n",
    "print(f\"   (b) Green Day: {green_day_ppl}\")\n",
    "print(f\"   (c) Ed Sheeran: {ed_sheeran_ppl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cae22ef3-efa9-46b3-97fc-3f2acc6cf93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Most likely lyricist (Tri-Gram): Taylor Swift\n"
     ]
    }
   ],
   "source": [
    "# Identify the most likely lyricist based on lowest perplexity\n",
    "most_likely_artist = min(\n",
    "    [\"Taylor Swift\", \"Green Day\", \"Ed Sheeran\"],\n",
    "    key=lambda x: taylor_swift_ppl if x == \"Taylor Swift\" else green_day_ppl if x == \"Green Day\" else ed_sheeran_ppl\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Most likely lyricist (Tri-Gram): {most_likely_artist}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db076f7-e1de-44ec-a9c6-1743ab4531e2",
   "metadata": {},
   "source": [
    "## Task 2: Task 3: Compute Perplexity Scores for Bi-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ede0d95e-76e5-4a10-91e9-cb6fc666e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Perplexity scores for bi-gram models:\n",
      "   (a) Taylor Swift: 90.36503789935158\n",
      "   (b) Green Day: 286.38891953790426\n",
      "   (c) Ed Sheeran: 298.3478048232207\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the unattributed song (Bi-Gram)\n",
    "test_lyrics_bi = preprocess(read_file(\"test_lyrics.txt\"), n=2)\n",
    "\n",
    "# Train bi-gram models for each artist\n",
    "taylor_swift_train_bi = preprocess(read_file(\"taylor_swift.txt\"), n=2)\n",
    "green_day_train_bi = preprocess(read_file(\"green_day.txt\"), n=2)\n",
    "ed_sheeran_train_bi = preprocess(read_file(\"ed_sheeran.txt\"), n=2)\n",
    "\n",
    "# Create bi-gram language models\n",
    "taylor_swift_lm_bi = NGramLanguageModel(n=2, train_data=taylor_swift_train_bi, alpha=0.1)\n",
    "green_day_lm_bi = NGramLanguageModel(n=2, train_data=green_day_train_bi, alpha=0.1)\n",
    "ed_sheeran_lm_bi = NGramLanguageModel(n=2, train_data=ed_sheeran_train_bi, alpha=0.1)\n",
    "\n",
    "# Compute perplexity for each artist (Bi-Gram)\n",
    "taylor_swift_ppl_bi = taylor_swift_lm_bi.perplexity(test_lyrics_bi)\n",
    "green_day_ppl_bi = green_day_lm_bi.perplexity(test_lyrics_bi)\n",
    "ed_sheeran_ppl_bi = ed_sheeran_lm_bi.perplexity(test_lyrics_bi)\n",
    "\n",
    "print(\"\\n3. Perplexity scores for bi-gram models:\")\n",
    "print(f\"   (a) Taylor Swift: {taylor_swift_ppl_bi}\")\n",
    "print(f\"   (b) Green Day: {green_day_ppl_bi}\")\n",
    "print(f\"   (c) Ed Sheeran: {ed_sheeran_ppl_bi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae842212-51ed-4d24-a97a-79bf08a604de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Most likely lyricist (Bi-Gram): Taylor Swift\n"
     ]
    }
   ],
   "source": [
    "# Identify the most likely lyricist using Bi-Gram\n",
    "most_likely_artist_bi = min(\n",
    "    [\"Taylor Swift\", \"Green Day\", \"Ed Sheeran\"],\n",
    "    key=lambda x: taylor_swift_ppl_bi if x == \"Taylor Swift\" else green_day_ppl_bi if x == \"Green Day\" else ed_sheeran_ppl_bi\n",
    ")\n",
    "\n",
    "print(f\"\\n4. Most likely lyricist (Bi-Gram): {most_likely_artist_bi}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc211c88-fd39-4b20-9e35-a617ac4ef8d9",
   "metadata": {},
   "source": [
    "## Task 5: Compute Accuracy Scores for Bi-Gram & Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81ae03d2-08bf-44eb-8c29-aa950f1acb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min perplexity for model trained on billie_eillish.txt: billie_eillish.txt\n",
      "Min perplexity for model trained on ed_sheeran.txt: taylor_swift.txt\n",
      "Min perplexity for model trained on green_day.txt: taylor_swift.txt\n",
      "Min perplexity for model trained on taylor_swift.txt: taylor_swift.txt\n",
      "\n",
      "Accuracy score for 3-gram model: 50.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1️⃣ Set Parameters for the Model\n",
    "n = 3  # Change to 2 for Bi-Gram, 3 for Tri-Gram\n",
    "smoothing = 0.1\n",
    "\n",
    "# 2️⃣ Define the Path to the Lyrics Folder\n",
    "data_dir = Path(r\"C:\\Users\\STUDENT\\Downloads\\handout (3)\\handout\\data\\data\\lyrics\")\n",
    "\n",
    "# 3️⃣ Split Datasets into Train & Test\n",
    "train_splits, test_splits = split_and_save_datasets(data_dir=data_dir)\n",
    "\n",
    "# 4️⃣ Get All Text Files (Ignoring \"test_lyrics.txt\")\n",
    "files = [f for f in data_dir.glob(\"*.txt\") if f.name != \"test_lyrics.txt\"]\n",
    "\n",
    "# 5️⃣ Train and Evaluate Models\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for f in files:\n",
    "    # Load Training Data for Each Artist\n",
    "    train = train_splits[f.name]\n",
    "    \n",
    "    # Train the N-Gram Model\n",
    "    lm = NGramLanguageModel(n, train, smoothing)\n",
    "    \n",
    "    # Compute Perplexity Against All Other Artists\n",
    "    perplexities = {}\n",
    "    for f_2 in files:\n",
    "        test = test_splits[f_2.name]\n",
    "        ppl = lm.perplexity(test)\n",
    "        perplexities[f_2.name] = ppl\n",
    "    \n",
    "    # Identify the Most Likely Artist (Lowest Perplexity)\n",
    "    min_perplexity_file = min(perplexities, key=perplexities.get)\n",
    "\n",
    "    # 6️⃣ Check if the Model Correctly Identified the Artist\n",
    "    total += 1\n",
    "    if min_perplexity_file == f.name:\n",
    "        correct += 1\n",
    "\n",
    "    print(f\"Min perplexity for model trained on {f.name}: {min_perplexity_file}\")\n",
    "\n",
    "# 7️⃣ Compute Overall Accuracy\n",
    "accuracy = 100 * (correct / total)\n",
    "print(f\"\\nAccuracy score for {n}-gram model: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c96ec7",
   "metadata": {},
   "source": [
    "Below, contains helper code for questions 4.2.4 and 4.2.5. We use a helper function split_and_save_datasets, to save the train and test data for each artist separately. Then, we train an n-gram model for each artists train set, and evaluate its perplexity on each artist's test set. By checking the artist with the lowest perplexity, we can compute the accuracy of each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce9756ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min perplexity for model trained on billie_eillish.txt: billie_eillish.txt\n",
      "Min perplexity for model trained on ed_sheeran.txt: taylor_swift.txt\n",
      "Min perplexity for model trained on green_day.txt: taylor_swift.txt\n",
      "Min perplexity for model trained on taylor_swift.txt: taylor_swift.txt\n",
      "Accuracy score for 3-gram model:  50.0 %\n"
     ]
    }
   ],
   "source": [
    "# Modify this for your particular bi-gram or tri-gram model\n",
    "n = 3\n",
    "smoothing = 0.1\n",
    "min_freq = 2\n",
    "\n",
    "# Run the splitting and saving process for the lyrics data\n",
    "train_splits, test_splits = split_and_save_datasets(data_dir=\"data/lyrics/\")\n",
    "files = [f for f in Path(\"data/lyrics/\").glob(\"*.txt\") if f.name != \"test_lyrics.txt\"]\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for f in files:\n",
    "  # Train a n-gram for each artists' lyrics\n",
    "  train = train_splits[f.name]\n",
    "  lm = NGramLanguageModel(n, train, smoothing)\n",
    "\n",
    "  # Compute the model's perplexity on each other artist \n",
    "  perplexities = {}\n",
    "  for f_2 in files:\n",
    "    test = test_splits[f_2.name]\n",
    "    ppl = lm.perplexity(test)\n",
    "    perplexities[f_2.name] = ppl\n",
    "\n",
    "  # Which artist has the lowest perplexity for our model\n",
    "  min_perplexity_file = min(perplexities, key=perplexities.get)\n",
    "\n",
    "  # Check whether the lowest perplexity artist is the same as which our model \n",
    "  # was trained on. \n",
    "  total += 1\n",
    "  if min_perplexity_file == f.name:\n",
    "    correct += 1\n",
    "  \n",
    "  print(f\"Min perplexity for model trained on {f.name}: {min_perplexity_file}\")\n",
    "\n",
    "\n",
    "print(f\"Accuracy score for {n}-gram model: \", 100 * (correct / total), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749a49a0",
   "metadata": {
    "id": "749a49a0"
   },
   "source": [
    "### **Written 4.3.1** –  Intro to Decoding [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6949f6a5",
   "metadata": {},
   "source": [
    "Please take a look at and understand the functions: `best_candidate()`, `top_k_best_candidates()` and `generate_sentences_from_phrase()` in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8ed9b85",
   "metadata": {
    "id": "c8ed9b85"
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "smoothing = 0.1\n",
    "min_freq = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96b3d000",
   "metadata": {
    "id": "96b3d000"
   },
   "outputs": [],
   "source": [
    "train = read_file(\"data/lyrics/taylor_swift.txt\")\n",
    "train = preprocess(train, n)\n",
    "lm = NGramLanguageModel(n, train, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17794ab7",
   "metadata": {
    "id": "17794ab7"
   },
   "outputs": [],
   "source": [
    "s1 = (\"the\", \"tortured\", \"poets\", \"department\")\n",
    "\n",
    "s2 = (\"so\", \"long\", \"london\")\n",
    "\n",
    "s3 = (\"down\", \"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69ef66a2",
   "metadata": {
    "id": "69ef66a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('</s>', 1)\n"
     ]
    }
   ],
   "source": [
    "print(top_k_best_candidates(lm, s1, 5, without=['<s>', '</s>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb777afe-28b1-4f81-8907-8ea5862ad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/10, Loss: 2.5897959001072666\n",
      "Epoch 2/10, Loss: 1.9786226121894002\n",
      "Epoch 3/10, Loss: 1.8724897524848845\n",
      "Epoch 4/10, Loss: 1.8223498441743418\n",
      "Epoch 5/10, Loss: 1.7907547225442806\n",
      "Epoch 6/10, Loss: 1.7688029116316264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import loadfile, load_glove_embeddings, create_embedding_matrix\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"mps\" if torch.backends.mps.is_available() \n",
    "                                 else \"cuda\" if torch.cuda.is_available() \n",
    "                                 else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Embedding layer initialized with GloVe embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = False  # Freeze the embeddings\n",
    "\n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = x.to(self.device)\n",
    "        if hidden is None:\n",
    "            hidden = torch.zeros(1, x.size(0), self.hidden_dim).to(self.device)\n",
    "        else:\n",
    "            hidden = hidden.to(self.device)\n",
    "\n",
    "        # Embedding lookup\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # RNN forward pass\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
    "        self.eval()\n",
    "        predicted_sequence = []\n",
    "\n",
    "        # Convert the input sequence to indices, handling unknown words\n",
    "        sequence_indices = [word_to_ix.get(word, word_to_ix[UNK]) for word in sequence.split()]\n",
    "        sequence_tensor = torch.tensor(sequence_indices, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "\n",
    "        hidden = None\n",
    "        for _ in range(num_words):\n",
    "            with torch.no_grad():\n",
    "                # Forward pass\n",
    "                output, hidden = self.forward(sequence_tensor, hidden)\n",
    "                output = output[:, -1, :]  # Get the last output\n",
    "\n",
    "                # Predict the next word\n",
    "                if mode == 'max':\n",
    "                    _, next_word_idx = torch.max(output, dim=1)\n",
    "                elif mode == 'multinomial':\n",
    "                    probs = F.softmax(output, dim=1)\n",
    "                    next_word_idx = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "\n",
    "                # Get the predicted word\n",
    "                next_word = ix_to_word.get(next_word_idx.item(), UNK)\n",
    "                predicted_sequence.append(next_word)\n",
    "\n",
    "                # Update the sequence tensor with the predicted word\n",
    "                sequence_tensor = torch.tensor([[next_word_idx.item()]], dtype=torch.long).to(self.device)\n",
    "\n",
    "        return predicted_sequence\n",
    "\n",
    "# Load the dataset\n",
    "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/taylor_swift.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 128\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_path = \"glove.6B.50d.txt\"  # Update this path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Initialize the model\n",
    "model = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(model.device)\n",
    "        targets = targets.to(model.device)\n",
    "\n",
    "        # Forward pass\n",
    "        hidden = None\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.view(-1, model.vocab_size), targets.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "# Predict top 5 words for each track title\n",
    "s1 = (\"the\", \"tortured\", \"poets\", \"department\")\n",
    "s2 = (\"so\", \"long\", \"london\")\n",
    "s3 = (\"down\", \"bad\")\n",
    "\n",
    "print(\"s1:\", top_k_best_candidates(model, s1, k=5, without=[START, EOS]))\n",
    "print(\"s2:\", top_k_best_candidates(model, s2, k=5, without=[START, EOS]))\n",
    "print(\"s3:\", top_k_best_candidates(model, s3, k=5, without=[START, EOS]))\n",
    "\n",
    "# Generate a sentence using 'max' mode\n",
    "generated_sentence_max = model.generate_sentence(\"the tortured poets department\", word_to_ix, ix_to_word, num_words=10, mode='max')\n",
    "print(\"Generated sentence (max mode):\", generated_sentence_max)\n",
    "\n",
    "# Generate a sentence using 'multinomial' mode\n",
    "generated_sentence_multinomial = model.generate_sentence(\"the tortured poets department\", word_to_ix, ix_to_word, num_words=10, mode='multinomial')\n",
    "print(\"Generated sentence (multinomial mode):\", generated_sentence_multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4e956",
   "metadata": {
    "id": "8ee4e956"
   },
   "source": [
    "### **Written 4.3.2** – Text Generation [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205129a7",
   "metadata": {
    "id": "205129a7"
   },
   "source": [
    "For this subtask, train an RNN LM using `data/taylor_swift.txt`\n",
    "\n",
    "In this part, we will try the first two approaches to generate sentences.\n",
    "\n",
    "Q1. Use `predict_next_words()` method to generate sentences after the provided phrases from `s1` to `s3`. Use modes `max` and `multinomial`. Report one of your favorite generations (for any strategy or phrase).\n",
    "\n",
    "Q2. Which decoding strategy did you like better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb85208",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"the tortured poets department\"\n",
    "\n",
    "s2 = \"so long, london\"\n",
    "\n",
    "s3 = \"down bad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a19423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate your RNN model's perplexity\n",
    "torch.manual_seed(11411)\n",
    "\n",
    "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/taylor_swift.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "num_epochs = 10\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
    "\n",
    "perplexity = 0\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(RNN.device)\n",
    "        targets = targets.to(RNN.device)\n",
    "        \n",
    "        RNN.zero_grad()\n",
    "        output, _ = RNN(inputs)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    perplexity = np.exp(loss.item())\n",
    "\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3a291",
   "metadata": {
    "id": "dcd3a291"
   },
   "outputs": [],
   "source": [
    "sentence = s1\n",
    "predicted_words_sequence = RNN.generate_sentence(sentence, word_to_ix, ix_to_word, 10, mode='multinomial')\n",
    "print(sentence + ' ' + ' '.join(predicted_words_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712619f",
   "metadata": {
    "id": "d712619f"
   },
   "source": [
    "**Aside (for fun!)**: Train your LM on Taylor Swift lyrics and generate the next hit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9047d",
   "metadata": {
    "id": "17b9047d"
   },
   "source": [
    "### **Written 4.4** – Battle of the LMs: GPT-2, Trigram and RNN [8 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d38359",
   "metadata": {
    "id": "41d38359"
   },
   "source": [
    "For this subtask, you will be generating text and comparing GPT-2 with your n-gram and RNN language models. \n",
    "\n",
    "Generative pretrained transformer (GPT) is a neural language model series created by OpenAI. The n-gram language model you trained has on average around 10K-20K parameters (`len(lm.model)`.) Compare that to the 175 billion parameters of GPT-3, which is likely much smaller than more recent iterations (though they don't tell us anymore)!\n",
    "\n",
    "Let's see how GPT-2 compares to the LMs you trained in Written 4.3.1 on the `data/bbc/tech-small.txt` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143e87c",
   "metadata": {
    "id": "e143e87c"
   },
   "outputs": [],
   "source": [
    "# Calculate your n-gram model's perplexity\n",
    "test = preprocess(read_file(\"data/bbc/tech-small.txt\"), 3)\n",
    "NGram = NGramLanguageModel(n=3, train_data=test)\n",
    "NGram.perplexity(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164e62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate your RNN model's perplexity\n",
    "torch.manual_seed(11411)\n",
    "\n",
    "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/bbc/tech-small.txt\")\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 32\n",
    "num_epochs = 10\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
    "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
    "\n",
    "lines = \"\"\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(RNN.device)\n",
    "        targets = targets.to(RNN.device)\n",
    "        \n",
    "        RNN.zero_grad()\n",
    "        output, _ = RNN(inputs)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
    "    lines += line + \"\\n\"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db542ceb",
   "metadata": {
    "id": "db542ceb"
   },
   "source": [
    "#### Computing GPT-2's perplexity on the test set\n",
    "\n",
    "You need to enable a GPU runtime from the Colab `Runtime` menu option (you can also use your computer if you have an accelerator). Go to `Runtime` → `Change Runtime Type` → `Hardware Accelerator (GPU)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ldf6ovg4B5Qc",
   "metadata": {
    "id": "Ldf6ovg4B5Qc"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "\n",
    "model_id = \"distilgpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67zwTHSI0hCC",
   "metadata": {
    "id": "67zwTHSI0hCC"
   },
   "outputs": [],
   "source": [
    "test = read_file(\"data/bbc/tech-small.txt\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wmQKbMXjDFNj",
   "metadata": {
    "id": "wmQKbMXjDFNj"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 100\n",
    "\n",
    "nlls = []\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "    trg_len = end_loc - i  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "giXGq0Z0DWdr",
   "metadata": {
    "id": "giXGq0Z0DWdr"
   },
   "outputs": [],
   "source": [
    "print(\"Perplexity using GPT2:\", ppl.item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
