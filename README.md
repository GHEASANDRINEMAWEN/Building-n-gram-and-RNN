# Building-n-gram-and-RNN

This project implements two language models: an N-Gram Model and a Recurrent Neural Network (RNN) Model. The N-Gram model includes Laplace Smoothing to handle unseen words.

Overview
N-Gram Model: Uses frequency-based probabilities to predict the next word.

Laplace Smoothing: Adjusts probabilities to handle unknown words.

RNN Model: Learns word sequences using a neural network approach.

Both models are trained on a given text corpus and evaluated based on their predictive accuracy.
